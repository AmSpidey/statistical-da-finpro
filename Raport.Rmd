---
title: "Raport"
output: html_notebook
---

ANALIZA ZMIENNYCH OBJAŚNIAJĄCYCH
Protein: Zmienne są typu ilościowego. VIF pokazuje że wybrane predyktory są nieistotnie skolerowane.
Cancer: Zmienne są typu ilościowego.
W obu zastosowałam Lasso. Predyktorów było dużo więcej niż obserwacji, więc chciałam usunąć te mniej znaczące. Oprócz tego VIF.
Często stosuję tzw. holdout method. Powód jest taki że nieco łatwiej dostrzec, czy mamy overfitting czy underfitting. Zachowałam to w tym skrypcie.

```{r}
# Here I will load the appropiate vectors and histograms. The script will later recompute them.
load("for_raport.RData")
load("nowakowska.RData")
```


```{r}
library(glmnet)
library(carData)
library(car)
library(MLmetrics)
library(MASS)
library("randomForest")
library(caret)
library(leaps)
library(pls)
```

```{r}
formul <- function (X) {
  predictors <- paste(names(X), collapse = "+")
  form <- as.formula(paste("Y ~", predictors))
  return(form)
}
```

```{r}
sep_train_test <- function (data.train) {
  # separate into train and test
  trDF <- as.data.frame(data.train)
  
  ## 75% of the sample size
  smp_size <- floor(0.75 * nrow(trDF))
  
  ## set the seed to make the partition reproducible
  set.seed(123)
  train_ind <- sample(seq_len(nrow(trDF)), size = smp_size)
  
  return(train_ind)
  }

sep_by_indx <- function (indxs, data) {
  train <- data[indxs, ]
  test <- data[-indxs, ]
  to_ret <- list("train" = train, "test" = test)
  return(to_ret)
}

categorical <- function (data.train) {
  d <- sapply(data.train, is.factor)
  return(any(d))
}

bic <- function (train) {
  n <- dim(train)[1]
  empty.mod <- lm(Y ~ 1, data = train)
  form <- formul(train[-ncol(train)])
  stepAic <- stepAIC(empty.mod, form, k = log(n))
  return(stepAic)
}

lasso_non_zeroes <- function (lasso_best, yX) {
  cs <- coef(lasso_best)
  nonzero <- (which(cs[,1]!=0) - 1)[-1]
  r <- rownames(as.data.frame(nonzero))
  return(cbind(yX[,nonzero], yX[ncol(yX)]))
}

quiet <- function(x) { 
  sink(tempfile()) 
  on.exit(sink()) 
  invisible(force(x)) 
}

```

```{r}

load("~/SADFinal/protein.RData")
data.train.prot <- data.train
data.test.prot <- data.test
load("~/SADFinal/cancer.RData")
data.train.canc <- data.train
data.test.canc <- data.test

set.seed(123)

# ANALIZA ZMIENNYCH OBJASNIAJCYCH:
# Zmienne sa typu ilosciowego zarowno w protein jak i w cancer. Zmiennych objasniajacych jest wiecej niz obserwacji (dane wysokowymiarowe)
# Ponizej sprawdzam czy ktoras ze zmiennych objasniajacych jest typu factor.
categorical(data.train.prot)
categorical(data.train.canc)
# Sprawdzmy czy brakuje jakichs wartosci.
any(sapply(data.train.prot, is.na))
any(sapply(data.train.canc, is.na))
# Tak tez moge szybko sprawdzic czy zmienne sa ilosciowe. Gdyby nie byly, byloby mniej wartosci unikalnych.
length(unique(data.train.prot))
length(unique(data.train.canc[,2]))
```

```{r}
# ---------------------PROTEIN

fullProtPred <- data.train.prot[,-ncol(data.train.prot)]
fullProtRes <- data.train.prot[,ncol(data.train.prot)]

train_indxs <- sep_train_test(data.train.prot)
separated <- sep_by_indx(train_indxs, data.train.prot)

train <- separated$train
test <- separated$test

data <- train[,-ncol(train)]
res <- train[,ncol(train)]
testData <- test[,-ncol(test)]
testRes <- test[,ncol(test)]

# lasso
lasso <- cv.glmnet(as.matrix(data), as.matrix(res), nfolds = 10)
lasso_best <- glmnet(as.matrix(data), as.matrix(res), lambda = lasso$lambda.min)

predTest <- predict(lasso_best, s = lasso$lambda.min, newx = as.matrix(testData))
predTrain <- predict(lasso_best, s = lasso$lambda.min, newx = as.matrix(data))
MSELassoProt <- MSE(predTest, testRes)
MSELassoProtTrain <- MSE(predTrain, res)

```

```{r}
# lasso cv

lassoCV <- cv.glmnet(as.matrix(fullProtPred), as.matrix(fullProtRes), nfolds = 10)
lasso_best <- glmnet(as.matrix(fullProtPred), as.matrix(fullProtRes), lambda = lassoCV$lambda.min)

lasso.prot.cv.err <- (lassoCV$cvm[lassoCV$lambda == lassoCV$lambda.min])
predTrain <- predict(lasso_best, fullProtPred)
lasso.prot.train.err <- MSE(predTrain, fullProtRes)

```


```{r}
# choosing most important 5 predictors with leapForward.
reg <- quiet(regsubsets(x = data.train.prot[, -ncol(data.train.prot)], y = data.train.prot[, ncol(data.train.prot)], nvmax = 5, really.big=T, method = "forward"))
sum <- summary(reg)
sum$rsq
predictors.protein <- names(coef(reg,5))[-1]
```

Tutaj zastosuję metodę której ostatecznie użyję do predykcji.

```{r}
# Trying to choose more predictors to see how it works out for us.

trCtrl = trainControl(method = "cv", number = 10)
reg100 <- quiet(train(x = data, y = res, tuneGrid = expand.grid(nvmax = c(1:100)), method = "leapForward", trControl = trCtrl))
# seems like the optimal number of predictors is 5 looking at RMSE statistics (I ran this around 10 times and got 6 parameters once). Let's choose 5


# Let's try out hold out method first.

predTrain <- predict(reg100, as.data.frame(train))
MSELM5ProteinTrain <- MSE(predTrain, res)
predTest <- predict(reg100, as.data.frame(test))
MSELM5ProteinTest <- MSE(predTest, testRes)

reg100final <- train(x = fullProtPred, y = fullProtRes, tuneGrid = expand.grid(nvmax = c(1:100)), method = "leapForward", trControl = trCtrl)
reg100cv <- ((reg100$results)[5, "RMSE"])^2
predTrain <- predict(reg100final, fullProtPred)
reg100trainmse <- MSE(predTrain, fullProtRes)
```

```{r}
# In the end, I decide to go with the 5 predictors. From MSE we can see that lasso tends to underfit and so does stepAIC. Let's see the VIF histogram!
predProt <- paste(predictors.protein, collapse = "+")
form <- as.formula(paste("Y ~", predProt))
lm5Prot <- lm(form, as.data.frame(data.train.prot))

# VIF HISTOGRAM FOR PROTEIN

hist(vif(lm5Prot))
histProt <- vif(lm5Prot)

```

Let's create a table with crossvalidation results of each method.

```{r}
m <- 2
protCVs <- data.frame(matrix(ncol = m, nrow = 1))
x <- c("lasso", "Leap forward")
colnames(protCVs) <- x
protCVs[1] <- lasso.prot.cv.err
protCVs[2] <- reg100cv
protCVs
```
I am choosing leapForward, even though stepAIC has much better results. Why? Because stepAIC shows overfitting and leap forward has much less overfitting (comparing MSE on train and test).

```{r}
pred.protein <- predict(reg100, data.test.prot)

```



```{r}
#-----------------CANCER
fullCancPred <- data.train.canc[,-ncol(data.train.canc)]
fullCancRes <- data.train.canc[,ncol(data.train.canc)]

train_indxs <- sep_train_test(data.train.canc)
separated <- sep_by_indx(train_indxs, data.train.canc)

train <- separated$train
test <- separated$test

data <- train[,-ncol(train)]
res <- train[,ncol(train)]
testData <- test[,-ncol(test)]
testRes <- test[,ncol(test)]

#lasso
lasso <- cv.glmnet(as.matrix(data), as.matrix(res), nfolds = 10, alpha = 1)
lasso_best <- glmnet(as.matrix(data), as.matrix(res), lambda = lasso$lambda.min, alpha = 1)

predTest <- predict(lasso_best, s = lasso$lambda.min, newx = as.matrix(testData))
predTrain <- predict(lasso_best, s = lasso$lambda.min, newx = as.matrix(data))
MSELassoCancTest <- MSE(predTest, testRes)
MSELassoCancTrain <- MSE(predTrain, res)

```

```{r}
# lasso cv

lassoCV <- cv.glmnet(as.matrix(fullCancPred), as.matrix(fullCancRes), nfolds = 10, alpha = 1)
lasso_best <- glmnet(as.matrix(fullCancPred), as.matrix(fullCancRes), lambda = lassoCV$lambda.min, alpha = 1)

lasso.canc.cv.err <- (lassoCV$cvm[lassoCV$lambda == lassoCV$lambda.min])

predTrain <- predict(lasso_best, s = lassoCV$lamba.min, newx = as.matrix(fullCancPred))
lasso.canc.train.err <- MSE(predTrain, fullCancRes)

```



```{r}
#ridge
ridge <- cv.glmnet(as.matrix(data), as.matrix(res), nfolds = 10, alpha = 0)
ridge_best <- glmnet(as.matrix(data), as.matrix(res), lambda = ridge$lambda.min, alpha = 0)

predTest <- predict(ridge_best, s = ridge$lambda.min, newx = as.matrix(testData))
predTrain <- predict(ridge_best, s = ridge$lambda.min, newx = as.matrix(data))
MSEridgeCanc <- MSE(predTest, testRes)
MSEridgeCancTrain <- MSE(predTrain, res)
min(ridge$cvm)
min(lasso$cvm)

```

```{r}
#ridge cv

ridgeCV <- cv.glmnet(as.matrix(fullCancPred), as.matrix(fullCancRes), nfolds = 10, alpha = 0)
ridge_best <- glmnet(as.matrix(fullCancPred), as.matrix(fullCancRes), lambda = ridgeCV$lambda.min, alpha = 0)

ridge.canc.cv.err <- (ridgeCV$cvm[ridgeCV$lambda == ridgeCV$lambda.min])

predTrain <- predict(ridge_best, s = ridgeCV$lamba.min, newx = as.matrix(fullCancPred))
ridge.canc.train.err <- MSE(predTrain, fullCancRes)

```

```{r}
# randomForest

cancerRandomForest <- randomForest(x = data, y = res)

pred <- predict(cancerRandomForest, newdata = testData)
predTrain <- predict(cancerRandomForest, newdata = data)
MSERanForCancer <- MSE(pred, testRes)
MSERanForCancerTrain <- MSE(predTrain, res)

```

```{r}
# random forest on whole training set to take out the mse

cancerRandomForest3 <- randomForest(x = fullCancPred, y = fullCancRes)
pseu.rf.canc.cv.err <- mean(cancerRandomForest3$mse)
predTrain <- predict(cancerRandomForest3, fullCancPred)
rf.canc.train.err <- MSE(predTrain, fullCancRes)

```

```{r}
# Most important 100 predictors chosen with lasso.
canc100_best <- glmnet(as.matrix(fullCancPred), as.matrix(fullCancRes), alpha = 1)
canc100_best$df
canc100_best$lambda[39]
canc100_best <- glmnet(as.matrix(fullCancPred), as.matrix(fullCancRes), lambda = 0.014, alpha = 1)
canc100_best$df
subData <- lasso_non_zeroes(canc100_best, as.data.frame(data.train.canc))
predictors.cancer <- colnames(subData[-101])

```

```{r}
# Vif for cancer
predCanc <- paste(predictors.cancer, collapse = "+")
form <- as.formula(paste("Y ~", predCanc))
lm100Canc <- lm(form, as.data.frame(data.train.canc))

# VIF HISTOGRAM FOR CANCER

hist(vif(lm100Canc))
histCanc <- vif(lm100Canc)
```

```{r}
# Table of cross-validation for Cancer

m <- 3
cancCVs <- data.frame(matrix(ncol = m, nrow = 1))
x <- c("lasso", "ridge", "random forest")
colnames(cancCVs) <- x
cancCVs[1] <- lasso.canc.cv.err
cancCVs[2] <- ridge.canc.cv.err
cancCVs[3] <- pseu.rf.canc.cv.err
cancCVs
```

Finally I decide to use lasso because it seems to be having smaller difference between train and test error relatively to ridge.

```{r}
pred.cancer <- predict(lasso_best, s = lassoCV$lamba.min, newx = as.matrix(data.test.canc))
```

```{r}
# Saving the histograms and predictors to separate files.
save(predictors.protein, pred.protein, pred.cancer, file = "nowakowska.RData")
save(histCanc, histProt, file = "for_raport.RData")

```




