---
title: "Raport"
output: html_notebook
---

Analizę	zmiennych	objaśniających.	Opisz,	jakiego	są	one	typu?
Protein: Zmienne są typu ilościowego. VIF pokazuje że wybrane predyktory są nieistotnie skolerowane.
Cancer: Zmienne są typu ilościowego.
W obu zastosowałam Lasso. Predyktorów było dużo więcej niż obserwacji, więc chciałam usunąć te mniej znaczące. Oprócz tego VIF.

```{r}
library(glmnet)
library(carData)
library(car)
library(MLmetrics)
library(MASS)
library("randomForest")
library(caret)
library(leaps)
library(pls)
```

```{r}
formul <- function (X) {
  predictors <- predyktory <- paste(names(X), collapse = "+")
  form <- as.formula(paste("Y ~", predictors))
  return(form)
}
```



```{r}
sep_train_test <- function (data.train) {
  # separate into train and test
  trDF <- as.data.frame(data.train)
  
  ## 75% of the sample size
  smp_size <- floor(0.75 * nrow(trDF))
  
  ## set the seed to make the partition reproducible
  set.seed(321)
  train_ind <- sample(seq_len(nrow(trDF)), size = smp_size)
  
  train <- trDF[train_ind, ]
  test <- trDF[-train_ind, ]
  to_ret <- list("train" = train, "test" = test)
  return(to_ret)
}

categorical <- function (data.train) {
  d <- sapply(data.train, is.factor)
  return(any(d))
}

bic <- function (train) {
  n <- dim(train)[1]
  empty.mod <- lm(Y ~ 1, data = train)
  form <- formul(train[-ncol(train)])
  stepAic <- stepAIC(empty.mod, form, k = log(n))
  return(stepAic)
}

lasso_non_zeroes <- function (lasso_best, yX) {
  cs <- coef(lasso_best)
  nonzero <- (which(cs[,1]!=0) - 1)[-1]
  r <- rownames(as.data.frame(nonzero))
  subData <- cbind(yX[,nonzero], yX[ncol(yX)])
  return(subData)
}

```

```{r}

load("~/SADFinal/protein.RData")
data.train.prot <- data.train
data.test.prot <- data.test
data.train.prot[1,]
# ---------------------PROTEIN

separated <- sep_train_test(data.train.prot)
categorical(data.train.prot) # there are no categorical values in protein.

train <- separated$train
test <- separated$test

data <- train[,-ncol(train)]
res <- train[,ncol(train)]
testData <- test[,-ncol(test)]
testRes <- test[,ncol(test)]

#lasso
lasso <- cv.glmnet(as.matrix(data), as.matrix(res), nfolds = 10)
lasso_best <- glmnet(as.matrix(data), as.matrix(res), lambda = lasso$lambda.min)
subData <- lasso_non_zeroes(lasso_best, as.data.frame(train))

#ridge
ridge <- cv.glmnet(as.matrix(data), as.matrix(res), alpha = 0, nfolds = 10)

pred <- predict(lasso_best, s = lasso$lambda.min, newx = as.matrix(testData))
predRidge <- predict(ridge, s = ridge$lambda.min, newx = as.matrix(testData))
predTrain <- predict(lasso_best, s = lasso$lambda.min, newx = as.matrix(data))
MSETrain <- MSE(predTrain, res)
MSELassoProt <- MSE(pred, testRes)
MSERidgeProt <- MSE(predRidge, testRes)
```


```{r}
# leap forward
leap <- train(data, res, method = "leapForward", tuneGrid = expand.grid(nvmax = 400))
pred <- predict(leap, newx = as.matrix(testData))
leap$levels
MSELeapForwardProt <- MSE(pred, testRes)
```


```{r}
# PLS: maybe will help us to avoid the issue of linear dependency

plsProt <- train(Y ~., data = train,
 method = "pls",
 tuneLength = 20,
 preProc = c("zv","center","scale"))


predPLS <- predict(plsProt, newx = testData)
MSEPLSProt <- MSE(predPLS, testRes)
#plsProt <- train(train[,-ncol(train)], train[,ncol(train)], method = "pls", tuneGrid = expand.grid(nvmax = seq(400)))

# linear dependencies
comboInfo <- findLinearCombos(data)
no.Depends <- data[, -comboInfo$remove]

# leap forward
leap <- train(no.Depends[,-ncol(no.Depends)], method = "leapForward", tuneGrid = expand.grid(nvmax = 5))
pred <- predict(leap, newx = as.matrix(testData[, -comboInfo$remove]))
MSELeapForwardProt <- MSE(pred, testRes)

```

```{r}

# lasso again
lassoNoDep <- cv.glmnet(as.matrix(no.Depends), as.matrix(res), nfolds = 10)
lassoNoDepBest <- glmnet(as.matrix(no.Depends), as.matrix(res), lambda = lassoNoDep$lambda.min)
predLassoNoDep <- predict(lassoNoDepBest, s = lassoNoDep$lambda.min, newx = as.matrix(testData[, -comboInfo$remove]))
MSELassoNoDepProt <- MSE(predLassoNoDep, testRes)

```


```{r}
# stepBIC
steps <- train(train[,-ncol(train)], train[,ncol(train)], method = "glmStepAIC")
empty.mod <- lm(Y ~ 1, data = subData)
form <- formul(subData[-ncol(subData)])
stepAic <- system.time(bic(subData))
stepAic
dim(as.data.frame(stepAic$coefficients))

```

```{r}
# randomForest
proteinRandomForest <- randomForest(x = data, y = res)

pred <- predict(proteinRandomForest, newdata = testData)
MSERanForProtein <- MSE(pred, testRes)
```


```{r}
# Vif
head(subData)
lm.vif = lm(Y ~ . , data = subData)
lm.vif$coefficients
vif(lm.vif)
outlierTest(lm.vif)
```

```{r}
#-----------------CANCER

load("~/SADFinal/cancer.RData")
data.train.canc <- data.train
data.test.canc <- data.test

separated <- sep_train_test(data.train.canc)
categorical(data.train.canc) # there are no categorical values in cancer.
train <- separated$train
test <- separated$test
data <- as.matrix(train[,-ncol(train)])
res <- as.matrix(train[,ncol(train)])
testData <- as.matrix(test[,-ncol(test)])
testRes <- as.matrix(test[,ncol(test)])

#lasso
lasso <- cv.glmnet(data, res, nfolds = 10, alpha = 1)
lasso_best <- glmnet(data, res, lambda = lasso$lambda.min)
subData <- lasso_non_zeroes(lasso_best, as.data.frame(train))
pred <- predict(lasso_best, s = lasso$lambda.min, newx = testData)
MSELassoCancer <- MSE(pred, testRes)

library(caret)
el.net <- train(train[,-ncol(train)], train[,ncol(train)], method = "glmnet", tuneLength = 10)

# ridge
ridgeC <- cv.glmnet(data, res, nfolds = 10, alpha = 0)
ridge.Best.C <- glmnet(data, res, lambda = ridgeC$lambda.min)
```

```{r}
# randomForest
cancerRandomForest <- randomForest(x = data, y = res)
t <- system.time(randomForest(x = data, y = res, ntree = 32))

pred <- predict(cancerRandomForest, newdata = testData)
head(pred)
MSERanForCancer <- MSE(pred, testRes)
MSERanForCancer

```

```{r}
# Let's try random forest on predictors from lasso. Random forest works better with carefully chosen predictors.
cancerRandomForest2 <- randomForest(x = subData[,-ncol(subData)], y = subData[,ncol(subData)])
pred <- predict(cancerRandomForest2, newdata = testData)
MSERanFor2Cancer <- MSE(pred, testRes)
importance(cancerRandomForest2)
```


```{r}


```


