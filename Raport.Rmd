---
title: "Raport"
output: html_notebook
---

ANALIZA ZMIENNYCH OBJAŚNIAJĄCYCH
Protein: Zmienne są typu ilościowego. VIF pokazuje że wybrane predyktory są nieistotnie skolerowane.
Cancer: Zmienne są typu ilościowego.
W obu zastosowałam Lasso. Predyktorów było dużo więcej niż obserwacji, więc chciałam usunąć te mniej znaczące. Oprócz tego VIF.

```{r}
library(glmnet)
library(carData)
library(car)
library(MLmetrics)
library(MASS)
library("randomForest")
library(caret)
library(leaps)
library(pls)
```

```{r}
formul <- function (X) {
  predictors <- paste(names(X), collapse = "+")
  form <- as.formula(paste("Y ~", predictors))
  return(form)
}
```

```{r}
sep_train_test <- function (data.train) {
  # separate into train and test
  trDF <- as.data.frame(data.train)
  
  ## 75% of the sample size
  smp_size <- floor(0.75 * nrow(trDF))
  
  ## set the seed to make the partition reproducible
  set.seed(123)
  train_ind <- sample(seq_len(nrow(trDF)), size = smp_size)
  
  return(train_ind)
  }

sep_by_indx <- function (indxs, data) {
  train <- data[indxs, ]
  test <- data[-indxs, ]
  to_ret <- list("train" = train, "test" = test)
  return(to_ret)
}

categorical <- function (data.train) {
  d <- sapply(data.train, is.factor)
  return(any(d))
}

bic <- function (train) {
  n <- dim(train)[1]
  empty.mod <- lm(Y ~ 1, data = train)
  form <- formul(train[-ncol(train)])
  stepAic <- stepAIC(empty.mod, form, k = log(n))
  return(stepAic)
}

lasso_non_zeroes <- function (lasso_best, yX) {
  cs <- coef(lasso_best)
  nonzero <- (which(cs[,1]!=0) - 1)[-1]
  r <- rownames(as.data.frame(nonzero))
  return(cbind(yX[,nonzero], yX[ncol(yX)]))
}

quiet <- function(x) { 
  sink(tempfile()) 
  on.exit(sink()) 
  invisible(force(x)) 
}

```

```{r}

load("~/SADFinal/protein.RData")
data.train.prot <- data.train
data.test.prot <- data.test
load("~/SADFinal/cancer.RData")
data.train.canc <- data.train
data.test.canc <- data.test

set.seed(123)

# ANALIZA ZMIENNYCH OBJASNIAJCYCH:
# Zmienne sa typu ilosciowego zarowno w protein jak i w cancer. Zmiennych objasniajacych jest wiecej niz obserwacji (dane wysokowymiarowe)
# Ponizej sprawdzam czy ktoras ze zmiennych objasniajacych jest typu factor.
categorical(data.train.prot)
categorical(data.train.canc)
# Sprawdzmy czy brakuje jakichs wartosci.
any(sapply(data.train.prot, is.na))
any(sapply(data.train.canc, is.na))
# Tak tez moge szybko sprawdzic czy zmienne sa ilosciowe. Gdyby nie byly, byloby mniej wartosci unikalnych.
length(unique(data.train.prot))
length(unique(data.train.canc[,2]))
```

```{r}
# ---------------------PROTEIN

train_indxs <- sep_train_test(data.train.prot)
separated <- sep_by_indx(train_indxs, data.train.prot)

train <- separated$train
test <- separated$test

data <- train[,-ncol(train)]
res <- train[,ncol(train)]
testData <- test[,-ncol(test)]
testRes <- test[,ncol(test)]

# lasso
lasso <- cv.glmnet(as.matrix(data), as.matrix(res), nfolds = 10)
lasso_best <- glmnet(as.matrix(data), as.matrix(res), lambda = lasso$lambda.min)

predTest <- predict(lasso_best, s = lasso$lambda.min, newx = as.matrix(testData))
predTrain <- predict(lasso_best, s = lasso$lambda.min, newx = as.matrix(data))
MSELassoProt <- MSE(predTest, testRes)
MSELassoProtTrain <- MSE(predTrain, res)

```

# vif dla lasso (potem porównam lasso z inną metodą by pokazać, dlaczego wybrałam lasso)
```{r}
subData <- lasso_non_zeroes(lasso_best, as.data.frame(data.train.prot))
sub <- sep_by_indx(train_indxs, subData)
                        
subTrain <- sub$train
subTest <- sub$test

lm.vif = lm(Y ~ . , data = subTrain)
lm.vif$coefficients
lassoVif <- vif(lm.vif)
hist(lassoVif)
```

```{r}
# stepAIC

empty.mod <- lm(Y ~ 1, data = subTrain)
all.mod <- lm(Y ~ ., data = subTrain)

stepaic <- quiet(stepAIC(empty.mod, direction="forward",
    scope=list(lower=empty.mod,
        upper=all.mod)
    ))
stepaic$residuals

predTrain <- predict(stepaic, newdata = sub$train)
MSEBICProt <- MSE(predTrain, res)
sub$subTest
predTest <- predict(stepaic, newdata = sub$test)
MSEBICProtTest <- MSE(predTest, testRes)

# Duzo wiekszy overfitting.

```


```{r}
# stepAIC cv
# I had problems using caret, so I implemented cross-validation myself.

k <- 10
cv_data <- subData
div <- cut(1:nrow(cv_data), 10, labels = FALSE)
div <- sample(div, length(div))
test.errs <- numeric(10)
train.errs <- numeric(10)

for (i in 1:k) {
  train_indx <- which(div != i)
  test_indx <- which(div == i)
  
  empty.mod <- lm(Y ~ 1, data = cv_data, subset = train_indx)
  all.mod <- lm(Y ~ ., data = cv_data, subset = train_indx)
  model <- quiet(stepAIC(empty.mod, direction="forward",
      scope=list(lower=empty.mod,
          upper=all.mod)
      ))
  
  train.err <- mean(model$residuals**2)
  
  pred <- predict(model, newdata=cv_data[test_indx, ])
  test.err <- mean((pred - cv_data[test_indx, "Y"])**2)
  test.errs[i] <- test.err
  train.errs[i] <- train.err
}

bic.prot.cv.err <- mean(test.errs)

```

```{r}
# lasso cv

lassoCV <- cv.glmnet(as.matrix(data.train.prot[, -ncol(data.train.prot)]), as.matrix(data.train.prot[, ncol(data.train.prot)]), nfolds = 10)
lasso.prot.cv.err <- lassoCV$cvm[lassoCV$lambda == lassoCV$lambda.min]

```

```{r}
# choosing most important 5 predictors with leapForward.
reg <- regsubsets(x = data.train.prot[, -ncol(data.train.prot)], y = data.train.prot[, ncol(data.train.prot)], nvmax = 5, really.big=T, method = "forward")
sum <- summary(reg)
predictors.protein <- names(coef(reg,5))[-1]
save(predictors.protein, file = "nowakowska.RData")
load("nowakowska.RData")
```


```{r}
#-----------------CANCER

separated <- sep_train_test(data.train.canc)

train <- separated$train
test <- separated$test

data <- train[,-ncol(train)]
res <- train[,ncol(train)]
testData <- test[,-ncol(test)]
testRes <- test[,ncol(test)]

#lasso
lasso <- cv.glmnet(as.matrix(data), as.matrix(res), nfolds = 10, alpha = 1)
lasso_best <- glmnet(as.matrix(data), as.matrix(res), lambda = lasso$lambda.min, alpha = 1)

predTest <- predict(lasso_best, s = lasso$lambda.min, newx = as.matrix(testData))
predTrain <- predict(lasso_best, s = lasso$lambda.min, newx = as.matrix(data))
MSELassoCanc <- MSE(predTest, testRes)
MSELassoCancTrain <- MSE(predTrain, res)

```


```{r}
#ridge
ridge <- cv.glmnet(as.matrix(data), as.matrix(res), nfolds = 10, alpha = 0)
ridge_best <- glmnet(as.matrix(data), as.matrix(res), lambda = ridge$lambda.min, alpha = 0)

predTest <- predict(ridge_best, s = ridge$lambda.min, newx = as.matrix(testData))
predTrain <- predict(ridge_best, s = ridge$lambda.min, newx = as.matrix(data))
MSEridgeCanc <- MSE(predTest, testRes)
MSEridgeCancTrain <- MSE(predTrain, res)
min(ridge$cvm)
min(lasso$cvm)

```


```{r}
# randomForest
cancerRandomForest <- randomForest(x = data, y = res)

pred <- predict(cancerRandomForest, newdata = testData)
predTrain <- predict(cancerRandomForest, newdata = data)
MSERanForCancer <- MSE(pred, testRes)
MSERanForCancerTrain <- MSE(predTrain, res)

```


```{r}
# Let's try random forest on predictors from lasso. Maybe Random Forest works better with carefully chosen predictors.
sub <- lasso_non_zeroes(lasso_best, data)
subTrain <- sub$subTrain

cancerRandomForest2 <- randomForest(x = subTrain[,-ncol(subTrain)], y = subTrain[,ncol(subTrain)])
pred <- predict(cancerRandomForest2, newdata = sub$subTest)
predTrain <- predict(cancerRandomForest2, newdata = sub$subTrain)
MSERanFor2Cancer <- MSE(pred, testRes)
MSERanForCancer2Train <- MSE(predTrain, res)
```


```{r}
# randomForest cv
k <- 10
cv_data <- data.train.canc
div <- cut(1:nrow(cv_data), 10, labels = FALSE)
div <- sample(div, length(div))
test.errs <- numeric(10)
train.errs <- numeric(10)

for (i in 1:k) {
  print(i)
  train_indx <- which(div != i)
  test_indx <- which(div == i)
  
  model <- randomForest(x = cv_data[train_indx, -ncol(cv_data)], y = cv_data[train_indx, ncol(cv_data)])
  #train.err <- mean(model$residuals**2)
  
  pred <- predict(model, newdata=cv_data[test_indx, ])
  test.err <- mean((pred - cv_data[test_indx, "Y"])**2)
  test.errs[i] <- test.err
  #train.errs[i] <- train.err
}

rf.canc.cv.err <- mean(test.errs)

```

```{r}
# random forest on whole training set to take out the mse

cancerRandomForest3 <- randomForest(x = data.train.canc[,-ncol(data.train.canc)], y = data.train.canc[,ncol(data.train.canc)])
pseu.rf.canc.cv.err <- mean(cancerRandomForest3$mse)

```

```{r}
# Most important 100 predictors chosen.
# First I removed linear dependencies so the model is easier to process.
comboInfo <- findLinearCombos(data.train.canc[-ncol(data.train.canc)])
indCanc <- data.train.canc[, -comboInfo$remove]

cancerLm <- lm (Y ~., data = indCanc)
stCancLm <- lm.beta(cancerLm)
coefs <- coef(stCancLm)
tail(sort((coefs)),100)


?leaps.default
```



